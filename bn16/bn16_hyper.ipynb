{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RaNzg2RIBQW"
   },
   "source": [
    "# BN16 Hyperparameter Optimization\n",
    "\n",
    "Playground for hyperparameter optimization for the 64-128-full-size CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BopKXkQNk6sA"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Jb-Ecdr_XtO5",
    "outputId": "938762dd-a27a-465a-efed-b14e4794371c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping dataclasses as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!TMPDIR=../../tmp pip install torch==1.7.* torch-summary numpy pandas matplotlib sparse ipywidgets\n",
    "#!TMPDIR=../../tmp pip install 'ray[tune]'\n",
    "!pip uninstall -y dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNYQvuF-Gpd9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary\n",
    "import sparse\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "import glob\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-06 18:28:32,853\tINFO services.py:1092 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_gpus=2)\n",
    "ray.get_gpu_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJA4BUyknSxx"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "i9UsNk_ZnV3k",
    "outputId": "6ef67640-7d84-423f-f860-baf457b5f553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(DEVICE)\n",
    "RND_SEED = 1\n",
    "BATCH_SIZE = 8\n",
    "N_STEPS = 12\n",
    "N_INDIVIDUALS = 2443\n",
    "\n",
    "torch.random.manual_seed(RND_SEED)\n",
    "torch.cuda.manual_seed(RND_SEED)\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.min_rows = None\n",
    "pd.options.display.width = 800\n",
    "np.set_printoptions(edgeitems=5)\n",
    "np.core.arrayprint._line_width = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aliveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_start_date = datetime.datetime(2016, 7, 23, 0, 0, 0, 0)\n",
    "hatching_start_date = datetime.datetime(2016, 7, 19, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_data = pd.read_csv('indices_bn16.csv').sort_values(by='bee_id')\n",
    "alive_data = pd.read_csv('alive_bn16.csv').sort_values(by='bee_id').reset_index(drop=True) # required or the data will be messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both\n",
    "combined_alive_data = alive_data\n",
    "combined_alive_data['interaction_matrix_idx'] = indices_data['interaction_matrix_idx']\n",
    "# now sort by matrix id for easy vector generation\n",
    "combined_alive_data = combined_alive_data.sort_values(by='interaction_matrix_idx')\n",
    "# date column type fix\n",
    "combined_alive_data['date_emerged'] = pd.to_datetime(combined_alive_data['date_emerged'])\n",
    "# let's translate the hatch day into a step where observation_start_date is step 0\n",
    "combined_alive_data['born_in_step'] = combined_alive_data['date_emerged'].map(lambda x: (x - observation_start_date).days * 48)\n",
    "combined_alive_data['dead_by_step'] = combined_alive_data.apply(lambda row: 48 * row['days_alive'] + row['born_in_step'], axis=1)\n",
    "# combined_alive_data = get_combined_alive_data(indices_data, alive_data, observation_start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data insights like age distribution take a look at the `insights` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bee_id_to_matrix_id(bee_id, index_data):\n",
    "    '''Translates the bee ID into the matrix ID.'''\n",
    "    return indices_data[indices_data['bee_id'] == bee_id]['interaction_matrix_idx'].item()\n",
    "\n",
    "def matrix_id_to_bee_id(matrix_id):\n",
    "    '''Translates the matrix ID into the bee ID.'''\n",
    "    return indices_data[indices_data['interaction_matrix_idx'] == matrix_id]['bee_id'].item()\n",
    "\n",
    "def get_aliveness_vector(step):\n",
    "    '''Returns a #individuals-long boolean vector of matrix-ID-indexed bee aliveness.'''\n",
    "    return (combined_alive_data['born_in_step'] <= step) & (step < combined_alive_data['dead_by_step'])\n",
    "\n",
    "def get_lifespan_for_row(row, step):\n",
    "    if ((step < row['born_in_step']) | (row['dead_by_step'] < step)): #iloc[0] if you pass in a df\n",
    "        return 0\n",
    "    return row['dead_by_step'] - step  #iloc[0] if you pass in a df    \n",
    "\n",
    "def get_lifespan_vector(step):\n",
    "    '''Returns a #individuals-long vector of matrix-ID-indexed remaining bee lifespans in steps (half an hour).'''\n",
    "    return combined_alive_data.apply(get_lifespan_for_row, axis=1, step=step)\n",
    "\n",
    "def get_lifespan_for_individual(midx):\n",
    "    '''Get the remaining steps to live for given individual matrix index.'''\n",
    "    # start at zero, not before\n",
    "    bis = max(combined_alive_data[combined_alive_data['interaction_matrix_idx'] == midx]['born_in_step'].item(), 0)\n",
    "    dis = combined_alive_data.iloc[midx]['dead_by_step']\n",
    "    # full range, 56 days Ã  48 steps\n",
    "    r = np.zeros(56 * 48) \n",
    "    # dead in 4 steps => 4,3,2,1 - clip to length\n",
    "    ins = np.arange(dis - bis)[::-1][:len(r) - bis] \n",
    "    r[bis:bis + len(ins)] = ins\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circadian Rhythmicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "circadian_data = pd.read_csv(\n",
    "    'rhythmicity_bn16.csv',\n",
    "    dtype={'bee_id': np.int16, 'age': np.int8, 'date': str, 'circadian_rhythmicity': np.float64},\n",
    "    parse_dates=['date'],\n",
    "    date_parser = pd.to_datetime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to expand the days into 48 copies, also, we don't know if there are gaps in the date so we should not rely on the date to be continous\n",
    "def circadian_vector_for_individual(bee_id, circadian, observation_start_date):\n",
    "    '''Get the vector of circadian rhythmicity per step for a bee.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bee_id : int\n",
    "        The ID of the bee.\n",
    "    circadian : pd.DataFrame\n",
    "        The data frame with the circadian rhythmicities.\n",
    "    observation_start_date: datetime\n",
    "        The datetime of observation start.\n",
    "    '''\n",
    "    v = np.zeros(56 * 48)\n",
    "    for row in circadian[circadian['bee_id'] == bee_id].itertuples():\n",
    "        ins_start = (row.date - observation_start_date).days * 48\n",
    "        v[ins_start:ins_start + 48] = np.repeat(row.circadian_rhythmicity, 48)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualDataSet(Dataset):\n",
    "    '''Holds the data for one individual.'''\n",
    "    def __init__(self, day, transform=None):\n",
    "        self.data = sparse.load_npz('/home/mi/rbergmann/ma/bn16/individuals/{}.npz'.format(day))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        d = torch.from_numpy(self.data[idx].todense())\n",
    "        if self.transform:\n",
    "            d = self.transform(d)\n",
    "        return d\n",
    "    \n",
    "class Bn16DataSet(Dataset):\n",
    "    def __init__(self, days=(0, 56), transform=None):\n",
    "        self.days = days\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2443\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        d = sparse.load_npz('/home/mi/rbergmann/ma/bn16/individuals/{}.npz'.format(idx))\n",
    "        d = d[self.days[0] * 48:self.days[1] * 48]\n",
    "        d = torch.from_numpy(d.todense().astype(np.float32))\n",
    "        if self.transform:\n",
    "            d = self.transform(d)\n",
    "        return (\n",
    "            d,\n",
    "            {\n",
    "                'lifespan': get_lifespan_for_individual(idx).astype('float'),\n",
    "                'circadian': circadian_vector_for_individual(\n",
    "                    matrix_id_to_bee_id(idx),\n",
    "                    circadian_data,\n",
    "                    observation_start_date\n",
    "                ).astype('float')\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 36 days\n",
    "bn16_data = Bn16DataSet(days=(0, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split\n",
    "train_data, test_data = random_split(bn16_data, [1955, 488])\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    bn16_data\n",
    "    ,batch_size=BATCH_SIZE\n",
    "    ,shuffle=True\n",
    "    ,num_workers=4\n",
    "    ,drop_last=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data\n",
    "    ,batch_size=BATCH_SIZE\n",
    "    ,shuffle=True\n",
    "    ,num_workers=4\n",
    "    ,drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data\n",
    "    ,batch_size=BATCH_SIZE\n",
    "    ,shuffle=True\n",
    "    ,num_workers=2\n",
    "    ,drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self, n_steps, batch_size, hidden_size=256, context_size=256):\n",
    "        super(CPC, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_size = context_size\n",
    "        self.g_enc = self.encoder()\n",
    "        self.g_ar = self.autoregressive()\n",
    "        self.step_W = nn.ModuleList([nn.Linear(self.context_size, self.hidden_size) for i in range(n_steps)])\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.cpc_target = torch.arange(0, batch_size).long().to(DEVICE)\n",
    "\n",
    "    def encoder(self):            \n",
    "        # conv2d signature: in, out, kernel, stride, padding\n",
    "        # input is: batch, channel, time, interactions\n",
    "        return nn.Sequential(            \n",
    "            # positional only : ~ 25% CPC\n",
    "            nn.Conv2d(1, self.hidden_size, (1, 2443), 1, 0), # positional filter\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def autoregressive(self):\n",
    "        return nn.GRU(self.hidden_size, self.context_size, batch_first=True)\n",
    "\n",
    "    def predict(self, x, hidden=None):\n",
    "        '''Return c_t for the whole given x, all its steps and the hidden state.'''\n",
    "        with torch.no_grad():\n",
    "            if hidden == None:\n",
    "                hidden = self.empty_hidden()\n",
    "            z = self.g_enc(x)\n",
    "            z = z.squeeze().permute(0, 2, 1)\n",
    "            pred, hidden = self.g_ar(z, hidden)\n",
    "            return pred[:,-1,:], pred, hidden\n",
    "\n",
    "#     @autocast()\n",
    "    def forward(self, x, gar_hidden):\n",
    "        # put through g_enc\n",
    "        z = self.g_enc(x) # for convs\n",
    "        # sample a subsequence of n steps\n",
    "        # but we need at least one bit to feed into the GRU, so we pick from 1 to seqlen - n_steps\n",
    "        subsample_idx = torch.randint(1, z.shape[1] - self.n_steps, size=(1,)) # 1-1680-12 = 1667\n",
    "        \n",
    "        # put everything before our pick through g_ar, collect c_t        \n",
    "        # g_ar expects shape(batch, seq_len, input_size), h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.GRU.html#torch.nn.GRU\n",
    "        # https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/\n",
    "        z = z.squeeze().permute(0, 2, 1)\n",
    "        c_tx, gar_hidden = self.g_ar(z[:,:subsample_idx,:], gar_hidden)\n",
    "        c_t = c_tx[:,-1,:].squeeze() # GRU outputs a c for every step - we want the last\n",
    "\n",
    "        # for n steps, put through W_k for corresponding step to get predictions\n",
    "        # W_k * c_t\n",
    "        pred = [self.step_W[step](c_t) for step in range(self.n_steps)]\n",
    "\n",
    "        # info nce loss\n",
    "        acc = []\n",
    "        info_nce_loss = []\n",
    "        for time_step in range(self.n_steps):\n",
    "            # f_k = exp(z_{t+k} * W_k * c_t)\n",
    "            f_k = torch.mm(z[:,subsample_idx + time_step,:].squeeze(), torch.transpose(pred[time_step], 0, 1))\n",
    "            info_nce_loss.append(self.loss_criterion(f_k, self.cpc_target))\n",
    "            accuracy = torch.mean((torch.argmax(f_k, dim=1) == self.cpc_target).float())\n",
    "            acc.append(accuracy)\n",
    "        info_nce_loss = torch.stack(info_nce_loss).mean()\n",
    "        \n",
    "        return torch.mean(info_nce_loss), torch.mean(torch.tensor(acc))\n",
    "    \n",
    "    def empty_hidden(self):\n",
    "        return torch.zeros(1, self.batch_size, self.context_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir='/home/mi/rbergmann/ma/bn16/'):\n",
    "    data_loader = full_loader\n",
    "    model = CPC(config['n_steps'], BATCH_SIZE, hidden_size=config['hidden_size'], context_size=config['context_size']).to(DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=config['eps'], weight_decay=0)\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    for epoch in range(50):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(data_loader):\n",
    "            time_batch_start = timer()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # add channel dimension - for convolutions\n",
    "            if len(x.shape) < 4:\n",
    "                x = x.unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            # fwd - info_nce loss inside forward\n",
    "            loss, accuracy = model(x, model.empty_hidden())\n",
    "\n",
    "            # back\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(accuracy)\n",
    "\n",
    "        epoch_loss_mean = torch.mean(torch.tensor(epoch_loss, dtype=torch.float64)).item()\n",
    "        epoch_acc_mean = torch.mean(torch.tensor(epoch_acc, dtype=torch.float64)).item()\n",
    "        # epoch report\n",
    "        print('Epoch: {} [{}/{} (100%)]\\tEpoch Loss: {:.8f}\\tEpoch Acc: {:.2f}%'.format(\n",
    "            epoch,\n",
    "            len(data_loader.dataset),\n",
    "            len(data_loader.dataset),\n",
    "            epoch_loss_mean,\n",
    "            epoch_acc_mean * 100\n",
    "        ))\n",
    "        \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=epoch_loss[-1], accuracy=epoch_acc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - CPC\n",
    "\n",
    "Actual training of the CPC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_size\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "    \"context_size\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "    \"n_steps\": tune.choice(list(range(1, 17))),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    #\"betas\": tune.loguniform(0.9, 0.9999),\n",
    "    \"eps\": tune.loguniform(1e-09, 1e-06),\n",
    "}\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=50,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tune.run(\n",
    "    train,\n",
    "    resources_per_trial={\"gpu\": 2},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=False, # happens within train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.checkpoint.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CPC Librispeech.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0156e8d0482540eca99e27edc2bdb261": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b80aa0abeba41d7891f8f6597058091": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "589fa440f5934454b099db6e0aa8c850": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86c07288905449cbb1240186b334eef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b80aa0abeba41d7891f8f6597058091",
      "max": 6387309499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0156e8d0482540eca99e27edc2bdb261",
      "value": 6387309499
     }
    },
    "958dfc4dc9314ae9b63f5a40aa1d0abf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b765884f57ee47efb8d5bc1a3b387faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_589fa440f5934454b099db6e0aa8c850",
      "placeholder": "â",
      "style": "IPY_MODEL_958dfc4dc9314ae9b63f5a40aa1d0abf",
      "value": " 5.95G/5.95G [05:26&lt;00:00, 19.5MB/s]"
     }
    },
    "cfe9a8d890474b61887f2ecb2997dccd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7c2e08c2fdd41d1acc7ad877708b5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_86c07288905449cbb1240186b334eef5",
       "IPY_MODEL_b765884f57ee47efb8d5bc1a3b387faa"
      ],
      "layout": "IPY_MODEL_cfe9a8d890474b61887f2ecb2997dccd"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
